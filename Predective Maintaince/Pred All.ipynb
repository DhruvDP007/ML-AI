{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     thermal  vibrational   acoustic  failure\n",
      "0  79.967142     1.699678  56.624109        1\n",
      "1  73.617357     1.462317  59.277407        1\n",
      "2  81.476885     1.029815  56.037900        0\n",
      "3  90.230299     0.676532  58.460192        1\n",
      "4  72.658466     1.349112  50.531927        0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic sensor data\n",
    "thermal_data = np.random.normal(loc=75, scale=10, size=n_samples)  # Mean temperature of 75Â°C, standard deviation of 10\n",
    "vibrational_data = np.random.normal(loc=1.0, scale=0.5, size=n_samples)  # Mean vibration of 1.0, standard deviation of 0.5\n",
    "acoustic_data = np.random.normal(loc=60, scale=5, size=n_samples)  # Mean acoustic level of 60 dB, standard deviation of 5\n",
    "\n",
    "# Generate binary target variable: 0 for no failure, 1 for failure\n",
    "# Failures are randomly assigned with a 20% probability\n",
    "failure = np.random.binomial(1, 0.2, n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'thermal': thermal_data,\n",
    "    'vibrational': vibrational_data,\n",
    "    'acoustic': acoustic_data,\n",
    "    'failure': failure\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data[['thermal', 'vibrational', 'acoustic']]\n",
    "y = data['failure']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[237   6]\n",
      " [ 56   1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.88       243\n",
      "           1       0.14      0.02      0.03        57\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.48      0.50      0.46       300\n",
      "weighted avg       0.68      0.79      0.72       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model Confusion Matrix:\n",
      "[[237   6]\n",
      " [ 56   1]]\n",
      "\n",
      "Loaded Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.88       243\n",
      "           1       0.14      0.02      0.03        57\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.48      0.50      0.46       300\n",
      "weighted avg       0.68      0.79      0.72       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'predictive_maintenance_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Load the model\n",
    "loaded_model = joblib.load('predictive_maintenance_model.pkl')\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Test the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_scaled)\n",
    "print(\"Loaded Model Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_loaded))\n",
    "print(\"\\nLoaded Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_loaded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Best Parameters: {'C': 0.01, 'penalty': 'l2'}\n",
      "Logistic Regression Best Score: 0.8028571428571428\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg_params = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "log_reg_grid = GridSearchCV(log_reg, log_reg_params, cv=5, scoring='accuracy')\n",
    "log_reg_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"Logistic Regression Best Parameters:\", log_reg_grid.best_params_)\n",
    "print(\"Logistic Regression Best Score:\", log_reg_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}\n",
      "Decision Tree Best Score: 0.7428571428571429\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "dt_grid = GridSearchCV(decision_tree, dt_params, cv=5, scoring='accuracy')\n",
    "dt_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"Decision Tree Best Parameters:\", dt_grid.best_params_)\n",
    "print(\"Decision Tree Best Score:\", dt_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Parameters: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Random Forest Best Score: 0.7985714285714286\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "random_forest = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "rf_grid = GridSearchCV(random_forest, rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"Random Forest Best Parameters:\", rf_grid.best_params_)\n",
    "print(\"Random Forest Best Score:\", rf_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Best Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
      "GBM Best Score: 0.8028571428571428\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "gbm_grid = GridSearchCV(gbm, gbm_params, cv=5, scoring='accuracy')\n",
    "gbm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"GBM Best Parameters:\", gbm_grid.best_params_)\n",
    "print(\"GBM Best Score:\", gbm_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Best Parameters: {'C': 0.01, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "SVM Best Score: 0.8028571428571428\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "svm = SVC()\n",
    "svm_params = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='accuracy')\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"SVM Best Parameters:\", svm_grid.best_params_)\n",
    "print(\"SVM Best Score:\", svm_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'solver': 'adam'}\n",
      "MLP Best Score: 0.8028571428571428\n"
     ]
    }
   ],
   "source": [
    "# Define the model and parameters\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "mlp_grid = GridSearchCV(mlp, mlp_params, cv=5, scoring='accuracy')\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters and performance\n",
    "print(\"MLP Best Parameters:\", mlp_grid.best_params_)\n",
    "print(\"MLP Best Score:\", mlp_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Performance:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "Decision Tree Test Performance:\n",
      "[[223  20]\n",
      " [ 51   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       243\n",
      "           1       0.23      0.11      0.14        57\n",
      "\n",
      "    accuracy                           0.76       300\n",
      "   macro avg       0.52      0.51      0.50       300\n",
      "weighted avg       0.70      0.76      0.73       300\n",
      "\n",
      "Random Forest Test Performance:\n",
      "[[242   1]\n",
      " [ 57   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.40      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "GBM Test Performance:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "SVM Test Performance:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n",
      "MLP Test Performance:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       243\n",
      "           1       0.00      0.00      0.00        57\n",
      "\n",
      "    accuracy                           0.81       300\n",
      "   macro avg       0.41      0.50      0.45       300\n",
      "weighted avg       0.66      0.81      0.72       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Logistic Regression\n",
    "y_pred_log_reg = log_reg_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"Logistic Regression Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_log_reg))\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "# Evaluate Decision Tree\n",
    "y_pred_dt = dt_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"Decision Tree Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Evaluate Random Forest\n",
    "y_pred_rf = rf_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"Random Forest Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# Evaluate GBM\n",
    "y_pred_gbm = gbm_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"GBM Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_gbm))\n",
    "print(classification_report(y_test, y_pred_gbm))\n",
    "\n",
    "# Evaluate SVM\n",
    "y_pred_svm = svm_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"SVM Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Evaluate MLP\n",
    "y_pred_mlp = mlp_grid.best_estimator_.predict(X_test_scaled)\n",
    "print(\"MLP Test Performance:\")\n",
    "print(confusion_matrix(y_test, y_pred_mlp))\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Decision Tree Performance:\n",
      "Accuracy: 0.7633\n",
      "Precision: 0.2308\n",
      "Recall: 0.1053\n",
      "F1 Score: 0.1446\n",
      "\n",
      "Random Forest Performance:\n",
      "Accuracy: 0.8067\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Gradient Boosting Machine Performance:\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "SVM Performance:\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "MLP Performance:\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Best Performing Model: Decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Admin451\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define a dictionary to store performance metrics\n",
    "performance = {}\n",
    "\n",
    "# Logistic Regression\n",
    "y_pred_log_reg = log_reg_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
    "report_log_reg = classification_report(y_test, y_pred_log_reg, output_dict=True)\n",
    "performance['Logistic Regression'] = {\n",
    "    'accuracy': report_log_reg['accuracy'],\n",
    "    'precision': report_log_reg['1']['precision'],\n",
    "    'recall': report_log_reg['1']['recall'],\n",
    "    'f1_score': report_log_reg['1']['f1-score']\n",
    "}\n",
    "\n",
    "# Decision Tree\n",
    "y_pred_dt = dt_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "report_dt = classification_report(y_test, y_pred_dt, output_dict=True)\n",
    "performance['Decision Tree'] = {\n",
    "    'accuracy': report_dt['accuracy'],\n",
    "    'precision': report_dt['1']['precision'],\n",
    "    'recall': report_dt['1']['recall'],\n",
    "    'f1_score': report_dt['1']['f1-score']\n",
    "}\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf = rf_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "report_rf = classification_report(y_test, y_pred_rf, output_dict=True)\n",
    "performance['Random Forest'] = {\n",
    "    'accuracy': report_rf['accuracy'],\n",
    "    'precision': report_rf['1']['precision'],\n",
    "    'recall': report_rf['1']['recall'],\n",
    "    'f1_score': report_rf['1']['f1-score']\n",
    "}\n",
    "\n",
    "# GBM\n",
    "y_pred_gbm = gbm_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_gbm = confusion_matrix(y_test, y_pred_gbm)\n",
    "report_gbm = classification_report(y_test, y_pred_gbm, output_dict=True)\n",
    "performance['Gradient Boosting Machine'] = {\n",
    "    'accuracy': report_gbm['accuracy'],\n",
    "    'precision': report_gbm['1']['precision'],\n",
    "    'recall': report_gbm['1']['recall'],\n",
    "    'f1_score': report_gbm['1']['f1-score']\n",
    "}\n",
    "\n",
    "# SVM\n",
    "y_pred_svm = svm_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "report_svm = classification_report(y_test, y_pred_svm, output_dict=True)\n",
    "performance['SVM'] = {\n",
    "    'accuracy': report_svm['accuracy'],\n",
    "    'precision': report_svm['1']['precision'],\n",
    "    'recall': report_svm['1']['recall'],\n",
    "    'f1_score': report_svm['1']['f1-score']\n",
    "}\n",
    "\n",
    "# MLP\n",
    "y_pred_mlp = mlp_grid.best_estimator_.predict(X_test_scaled)\n",
    "conf_matrix_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "report_mlp = classification_report(y_test, y_pred_mlp, output_dict=True)\n",
    "performance['MLP'] = {\n",
    "    'accuracy': report_mlp['accuracy'],\n",
    "    'precision': report_mlp['1']['precision'],\n",
    "    'recall': report_mlp['1']['recall'],\n",
    "    'f1_score': report_mlp['1']['f1-score']\n",
    "}\n",
    "\n",
    "# Print performance metrics\n",
    "for model, metrics in performance.items():\n",
    "    print(f\"\\n{model} Performance:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Determine the best-performing model based on F1 Score\n",
    "best_model = max(performance, key=lambda m: performance[m]['f1_score'])\n",
    "print(f\"\\nBest Performing Model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Performance:\n",
      "Confusion Matrix:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Decision Tree Performance:\n",
      "Confusion Matrix:\n",
      "[[223  20]\n",
      " [ 51   6]]\n",
      "Accuracy: 0.7633\n",
      "Precision: 0.2308\n",
      "Recall: 0.1053\n",
      "F1 Score: 0.1446\n",
      "\n",
      "Random Forest Performance:\n",
      "Confusion Matrix:\n",
      "[[242   1]\n",
      " [ 57   0]]\n",
      "Accuracy: 0.8067\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "GBM Performance:\n",
      "Confusion Matrix:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "SVM Performance:\n",
      "Confusion Matrix:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "MLP Performance:\n",
      "Confusion Matrix:\n",
      "[[243   0]\n",
      " [ 57   0]]\n",
      "Accuracy: 0.8100\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Best Performing Model: Decision Tree\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define a dictionary to store performance metrics\n",
    "performance = {}\n",
    "\n",
    "def evaluate_model(model_name, model, X_test_scaled, y_test):\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "# Evaluate each model\n",
    "f1_scores = {}\n",
    "\n",
    "f1_scores['Logistic Regression'] = evaluate_model('Logistic Regression', log_reg_grid.best_estimator_, X_test_scaled, y_test)\n",
    "f1_scores['Decision Tree'] = evaluate_model('Decision Tree', dt_grid.best_estimator_, X_test_scaled, y_test)\n",
    "f1_scores['Random Forest'] = evaluate_model('Random Forest', rf_grid.best_estimator_, X_test_scaled, y_test)\n",
    "f1_scores['Gradient Boosting Machine'] = evaluate_model('GBM', gbm_grid.best_estimator_, X_test_scaled, y_test)\n",
    "f1_scores['SVM'] = evaluate_model('SVM', svm_grid.best_estimator_, X_test_scaled, y_test)\n",
    "f1_scores['MLP'] = evaluate_model('MLP', mlp_grid.best_estimator_, X_test_scaled, y_test)\n",
    "\n",
    "# Determine the best-performing model based on F1 Score\n",
    "best_model = max(f1_scores, key=lambda m: f1_scores[m])\n",
    "print(f\"\\nBest Performing Model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
